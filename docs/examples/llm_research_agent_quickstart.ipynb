{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Research Agent Quickstart\n",
        "\n",
        "This notebook demonstrates an end-to-end LLM research workflow using mocked\n",
        "services so it can run entirely offline. Follow the accompanying\n",
        ":doc:`../llm_research_agent` guide for environment setup and security notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configure the environment\n",
        "\n",
        "Set the environment variables that the agent relies on. In production you would\n",
        "load them from a secure store; here we use temporary in-memory values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ['ACE_AGENT_DATA_DIR'] = '/tmp/ace-agent/data'\n",
        "os.environ['ACE_AGENT_MODEL_CACHE'] = '/tmp/ace-agent/models'\n",
        "os.environ['ACE_AGENT_API_KEY'] = 'mock-api-key'\n",
        "os.environ['ACE_AGENT_RESULTS_BUCKET'] = 'file:///tmp/ace-agent/results'\n",
        "os.environ['ACE_AGENT_LOG_LEVEL'] = 'INFO'\n",
        "\n",
        "for key in sorted(k for k in os.environ if k.startswith('ACE_AGENT_')):\n",
        "    print(f\"{key}={os.environ[key]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Assemble a mocked dataset\n",
        "\n",
        "Real experiments would read from ``ACE_AGENT_DATA_DIR``. The snippet below\n",
        "creates a lightweight dataset for illustrative purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "DATASET_PATH = Path(os.environ['ACE_AGENT_DATA_DIR']) / 'prompts.json'\n",
        "DATASET_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "mock_dataset = [\n",
        "    {\n",
        "        'id': 'sample-001',\n",
        "        'prompt': 'Summarize the latest ocean temperature anomaly findings.',\n",
        "        'context': 'Global SST anomalies remain above the 1985-2014 baseline.'\n",
        "    },\n",
        "    {\n",
        "        'id': 'sample-002',\n",
        "        'prompt': 'Describe possible drivers of the observed Arctic amplification.',\n",
        "        'context': 'Sea-ice loss and increased poleward moisture transport are key.'\n",
        "    },\n",
        "]\n",
        "DATASET_PATH.write_text(json.dumps(mock_dataset, indent=2))\n",
        "print(f'Wrote {DATASET_PATH}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define a mocked LLM interface\n",
        "\n",
        "The project typically integrates with remote providers. For testing we inject a\n",
        "mock implementation that produces deterministic responses based on the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MockLLM:\n",
        "    name: str = 'mock-llm-v1'\n",
        "\n",
        "    def generate(self, prompt: str, context: str) -> str:\n",
        "        return (\n",
        "            f\"[{self.name}] Prompt: {prompt}\\n\"\n",
        "            f\"Context: {context}\\n\"\n",
        "            \"Response: Further analysis scheduled.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def run_agent(prompts: List[Dict[str, str]], llm: MockLLM):\n",
        "    results = []\n",
        "    for entry in prompts:\n",
        "        response = llm.generate(entry['prompt'], entry['context'])\n",
        "        results.append({\n",
        "            'id': entry['id'],\n",
        "            'response': response,\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "mock_llm = MockLLM()\n",
        "print(f'Initialized {mock_llm.name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Execute the research workflow\n",
        "\n",
        "Load the dataset, generate outputs, and persist the findings to the results\n",
        "directory specified in ``ACE_AGENT_RESULTS_BUCKET``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RESULTS_PATH = Path('/tmp/ace-agent/results')\n",
        "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with DATASET_PATH.open() as f:\n",
        "    prompts = json.load(f)\n",
        "\n",
        "results = run_agent(prompts, mock_llm)\n",
        "OUTPUT_FILE = RESULTS_PATH / 'mock_results.json'\n",
        "OUTPUT_FILE.write_text(json.dumps(results, indent=2))\n",
        "print(f'Saved {len(results)} results to {OUTPUT_FILE}')\n",
        "print('\nFirst response:\n', results[0]['response'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Next steps\n",
        "\n",
        "* Swap ``MockLLM`` for your production client implementation.\n",
        "* Replace the synthetic dataset with curated experiments stored in ``ACE_AGENT_DATA_DIR``.\n",
        "* Integrate evaluation metrics or visualization notebooks to compare experiment runs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}